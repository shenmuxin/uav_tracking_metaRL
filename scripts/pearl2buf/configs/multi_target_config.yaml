# PEARL direction target reward configs

# General setup
# -------------
# Number of tasks for meta-train
train_tasks: 3

# Number of tasks for meta-test
test_tasks: 2

# Dimension of the latent context vector
latent_dim: 5

# Dimension of hidden units in neural networks
hidden_dim: 300

# PEARL setup
# -----------
pearl_params:
    # Number of training iterations
    num_iterations: 300
    # Number of sampled tasks to collect data for each iteration
    num_sample_tasks: 2
    # Number of samples collected per task before training
    num_init_trajs: 5
    # Number of samples to collect per task with z ~ prior
    num_prior_trajs: 1
    # Number of samples to collect per task with z ~ posterior
    # that are only used to train the policy and NOT the encoder
    num_posterior_trajs: 1
    # Number of meta-gradient taken per iteration
    num_meta_grads: 1500
    # Number of task samples for training
    meta_batch_size: 4
    # Number of samples in the context batch
    batch_size: 256
    # Maximum step for the environment, 环境允许的最大交互步数，即一个trajectory的最大长度
    max_step: 260   # max step for my env is 270 
    # How many samples to store
    max_buffer_size: 1000000
    # Number of early stopping conditions
    num_stop_conditions: 3
    # Goal value used to early stopping condition
    stop_goal: 1900

# SAC setup
# ---------
sac_params:
    # Discount factor
    gamma: 0.99
    # Weight on KL divergence term in encoder loss
    kl_lambda: 0.1
    # Number of samples in the RL batch
    batch_size: 256
    # Q-function network's learning rate
    qf_lr: 0.0003
    # Encoder network's learning rate
    encoder_lr: 0.0003
    # Policy network's learning rate
    policy_lr: 0.0003
